{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB,CategoricalNB,ComplementNB,GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, download\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report,cohen_kappa_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import VotingClassifier,BaggingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from spacy import displacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nlp:\n",
    "    def __init__(self,X):\n",
    "        self.df = X\n",
    "        self.text_nltk = list()\n",
    "        self.text_spacy = list()\n",
    "        \n",
    "    def pre_processing(self,textos):\n",
    "        caracteres_1 = re.compile(\"[.;:!\\'?@,\\\"()\\[\\]]\")\n",
    "        caracteres_2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "        caracteres_3 = re.compile('[^A-Za-z0-9\\s]+')\n",
    "\n",
    "        textos = [caracteres_1.sub(\"\", texto) for texto in textos]\n",
    "        textos = [caracteres_2.sub(\" \", texto.lower()) for texto in textos]\n",
    "        textos = [caracteres_3.sub(\"\", texto) for texto in textos]\n",
    "\n",
    "        return textos\n",
    "\n",
    "    def processamento_nltk(self):    \n",
    "        text_processado = list()\n",
    "        df_processed = self.pre_processing(self.df)\n",
    "        df_processed = [word_tokenize(text) for text in df_processed]\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "        df_processed = [[token for token in text if token not in stop_words]\n",
    "                      for text in df_processed]\n",
    "\n",
    "        lematizer = WordNetLemmatizer()\n",
    "        df_processed = [[lematizer.lemmatize(token) for token in text] for text in df_processed]\n",
    "\n",
    "        stemization = LancasterStemmer()\n",
    "        df_processed = [[stemization.stem(token) for token in text] for text in df_processed]\n",
    "\n",
    "        for frase in df_processed:\n",
    "            text_processado.append(' '.join(frase))\n",
    "\n",
    "        return text_processado\n",
    "    \n",
    "    def processamento_spacy(self):\n",
    "        df_tokens = list()\n",
    "        df = self.pre_processing(self.df)\n",
    "        df_processado = list()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        for text in df:\n",
    "            tokens = list()\n",
    "            doc = nlp(text)\n",
    "            for token in doc:\n",
    "                if(not(token.is_stop)):\n",
    "                    tokens.append(token.lemma_)\n",
    "            df_tokens.append(tokens)\n",
    "\n",
    "        for frase in df_tokens:\n",
    "            df_processado.append(' '.join(frase))\n",
    "        return df_processado\n",
    "    \n",
    "    def limpesa(self):\n",
    "        self.text_nltk = self.processamento_nltk()\n",
    "        self.text_spacy = self.processamento_spacy()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"dataset/test.csv\")\n",
    "df = pd.concat([df_train,df_test])\n",
    "\n",
    "def sentiment_categorization(df):\n",
    "    df.loc[df.sentiment=='neg','sentiment_cate'] = 0\n",
    "    df.loc[df.sentiment=='pos','sentiment_cate'] = 1\n",
    "    return df\n",
    "\n",
    "df = sentiment_categorization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = list(df.text)\n",
    "y = df['sentiment_cate'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um objeto NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Nlp(df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamando o metodo Limpesa para realizar as operações de limpesa utilizando o nltk e o spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.limpesa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os textos limpos em um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text_processed_nltk'] = nlp.text_nltk\n",
    "#df['text_processed_spacy'] = nlp.text_spacy\n",
    "#df.to_csv('dataset/df_processado_atual.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classificadores:\n",
    "    def __init__(self,X,y):\n",
    "        self.X_train = list()\n",
    "        self.y_train = list()\n",
    "        self.X_test = list()\n",
    "        self.y_test = list()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    \n",
    "    def regression_logistical(self):\n",
    "        model_LR = LogisticRegression(solver='lbfgs', max_iter=500,random_state=42)\n",
    "        model_LR = model_LR.fit(self.X_train, self.y_train)\n",
    "        return model_LR \n",
    "    \n",
    "    def knn(self):\n",
    "        knn = KNeighborsClassifier(n_neighbors=100,weights='distance')\n",
    "        model_knn = knn.fit(self.X_train, self.y_train)\n",
    "        return model_knn\n",
    "    \n",
    "    def naive(self):\n",
    "        naive = MultinomialNB() \n",
    "        model_naive = naive.fit(self.X_train, self.y_train)\n",
    "        return model_naive\n",
    "    \n",
    "    def decision_tree(self):\n",
    "        tree = DecisionTreeClassifier(criterion = 'gini',random_state=42,max_depth=15)\n",
    "        model_tree = tree.fit(self.X_train, self.y_train)\n",
    "        return model_tree\n",
    "    \n",
    "    def random_forest(self):\n",
    "        random_forest = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1,random_state=42)\n",
    "        model_rf = random_forest.fit(self.X_train, self.y_train)\n",
    "        return model_rf\n",
    "        \n",
    "    def perceptron(self):\n",
    "        ptn = Perceptron(max_iter=500)                    \n",
    "        model_ptn = ptn.fit(self.X_train, self.y_train)  \n",
    "        return model_ptn\n",
    "    \n",
    "    def ensemble(self):\n",
    "        lr_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        naive_clf = MultinomialNB()\n",
    "        rf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1,random_state=42)\n",
    "        voting_clf = VotingClassifier(\n",
    "                     estimators=[('lr', lr_clf), ('naive', naive_clf), ('rf', rf_clf)],\n",
    "                     voting='hard')        \n",
    "        voting_clf = voting_clf.fit(self.X_train, self.y_train)\n",
    "        return voting_clf\n",
    "    \n",
    "    def bagging(self):\n",
    "        rf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1,random_state=42)\n",
    "        bag_clf = BaggingClassifier(rf_clf)\n",
    "        bag_clf = bag_clf.fit(self.X_train, self.y_train)\n",
    "        return bag_clf\n",
    "    \n",
    "    def vetorizer(self):\n",
    "        vetorizer = CountVectorizer()\n",
    "        self.X_train = vetorizer.fit_transform(self.X_train)\n",
    "        self.X_test = vetorizer.transform(self.X_test)\n",
    "\n",
    "        transformer = TfidfTransformer()\n",
    "        self.X_train = transformer.fit_transform(self.X_train)\n",
    "        self.X_test = transformer.transform(self.X_test)\n",
    "        \n",
    "    def treinamento(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size = 0.2, random_state = 42,stratify=y)\n",
    "        self.vetorizer()\n",
    "        \n",
    "        model_lr = self.regression_logistical()\n",
    "        model_knn = self.knn()\n",
    "        model_naive = self.naive()\n",
    "        model_tree = self.decision_tree()\n",
    "        model_rf = self.random_forest()\n",
    "        model_ptn = self.perceptron()\n",
    "        model_ensemble = self.ensemble()\n",
    "        model_bagging = self.bagging()\n",
    "        return model_lr, model_knn,  model_naive ,model_tree, model_rf ,model_ptn ,model_ensemble, model_bagging\n",
    "    \n",
    "    def salve_model(self,model,name_model):\n",
    "        filename = 'modelos/'+ name_model\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "        \n",
    "    def open_model(self,name_model):\n",
    "        filename = 'modelos/'+ name_model\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        return loaded_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_nltk = Classificadores(nlp.text_nltk,y)\n",
    "c_spacy = Classificadores(nlp.text_spacy,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_nltk = c_nltk.treinamento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_spacy = c_spacy.treinamento()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva os modelos em um diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_models = [\"model_lr\", \"model_knn\",  \"model_naive\" ,\"model_tree\", \"model_rf\" ,\"model_ptn\" ,\"model_ensemble\", \"model_bagging\"]\n",
    "for model,name in zip(models_nltk,names_models):\n",
    "    c_nltk.salve_model(model,name+\"_nltk\")\n",
    "\n",
    "for model,name in zip(models_spacy,names_models):\n",
    "    c_spacy.salve_model(model,name+\"_spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa modelos salvos de um diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_models = [\"model_lr\", \"model_knn\",  \"model_naive\" ,\"model_tree\", \"model_rf\" ,\"model_ptn\" ,\"model_ensemble\", \"model_bagging\"]\n",
    "for name in names_models:\n",
    "    models_nltk1 = c_nltk.open_model(name+\"_nltk\")\n",
    "\n",
    "for name in names_models:\n",
    "    models_spacy1 = c_spacy.open_model(name+\"_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptn = Perceptron(max_iter=500)                    \n",
    "model_ptn = ptn.fit(c_nltk.X_train, c_nltk.y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelos/ptn.pkl', 'wb') as file:\n",
    "    pickle.dump(model_ptn, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelos/ptn.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ptn == model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
