{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list(df_train.text[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(textos):\n",
    "    caracteres_1 = re.compile(\"[.;:!\\'?@,\\\"()\\[\\]]\")\n",
    "    caracteres_2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    caracteres_3 = re.compile('[^A-Za-z0-9\\s]+')\n",
    "\n",
    "    textos = [caracteres_1.sub(\"\", texto) for texto in textos]\n",
    "    textos = [caracteres_2.sub(\" \", texto.lower()) for texto in textos]\n",
    "    textos = [caracteres_3.sub(\"\", texto) for texto in textos]\n",
    "    \n",
    "    return textos\n",
    "\n",
    "df_processed = pre_processing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(df):\n",
    "    df_tokens = list()\n",
    "    for text in df:\n",
    "        tokens = list()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            tokens.append(token.text)\n",
    "        df_tokens.append(tokens)\n",
    "    return df_tokens\n",
    "\n",
    "df_processed = tokenizer(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens = list()\n",
    "for text in df_processed:\n",
    "    tokens = list()\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    df_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['now', 'i', 'wo', 'nt', 'deny', 'that', 'when', 'i', 'purchased', 'this', 'off', 'ebay', 'i', 'had', 'high', 'expectations', 'this', 'was', 'an', 'incredible', 'out', 'of', 'print', 'work', 'from', 'the', 'master', 'of', 'comedy', 'that', 'i', 'so', 'enjoy', 'however', 'i', 'was', 'soon', 'to', 'be', 'disappointed', 'apologies', 'to', 'those', 'who', 'enjoyed', 'it', 'but', 'i', 'just', 'found', 'the', 'compleat', 'al', 'to', 'be', 'very', 'difficult', 'to', 'watch', 'i', 'got', 'a', 'few', 'smiles', 'sure', 'but', 'the', 'majority', 'of', 'the', 'funny', 'came', 'from', 'the', 'music', 'videos', 'which', 'i', 've', 'got', 'on', 'dvd', 'and', 'the', 'rest', 'was', 'basically', 'filler', 'you', 'could', 'tell', 'that', 'this', 'was', 'not', 'als', 'greatest', 'video', 'achievement', 'that', 'honor', 'goes', 'to', 'uhf', 'honestly', 'i', 'doubt', 'if', 'this', 'will', 'ever', 'make', 'the', 'jump', 'to', 'dvd', 'so', 'if', 'you', 're', 'an', 'ultra', 'hardcore', 'al', 'fan', 'and', 'just', 'have', 'to', 'own', 'everything', 'buy', 'the', 'tape', 'off', 'ebay', 'just', 'do', 'nt', 'pay', 'too', 'much', 'for', 'it']\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-d56d16ced9bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"i have a car\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"i have a car\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "word_tokenize({[\"i have a car\",\"i have a car\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[5].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have to admit that i liked the first half of Sleepers. It looked good, the acting was even better, the story of childhood, pain and revenge was interesting and moving. A superior hollywood film. But...No one mentioned this so far (at least in the latest 20 comments), when it came to the courtroom scenes and Brat Pitt´s character followed his plan to rescue his two friends, who are rightly accused of murder, i felt cheated. This movie insulted my intelligence. <br /><br />Warning spoilers!!<br /><br />Why did anyone accept their false alibi, witnessed by the priest? If these two guys had been with him, why shouldn´t they tell this during the investigation? Amnesia? If you were the judge or member of the jury, would you believe it? Is it wise to give the motif of the murderers away?<br /><br />I am sorry, but in the end, the story is very weak, and this angers me. This movie had great potential. 4/10\n"
     ]
    }
   ],
   "source": [
    "print(df[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I PRON PRP nsubj X True True\n",
      "have have VERB VBP ROOT xxxx True True\n",
      "to to PART TO aux xx True True\n",
      "admit admit VERB VB xcomp xxxx True False\n",
      "that that SCONJ IN mark xxxx True True\n",
      "i I PRON PRP nsubj x True True\n",
      "liked like VERB VBD ccomp xxxx True False\n",
      "the the DET DT det xxx True True\n",
      "first first ADJ JJ amod xxxx True True\n",
      "half half NOUN NN dobj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "Sleepers Sleepers PROPN NNPS pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "It it PRON PRP nsubj Xx True True\n",
      "looked look VERB VBD ccomp xxxx True False\n",
      "good good ADJ JJ acomp xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "the the DET DT det xxx True True\n",
      "acting acting NOUN NN nsubj xxxx True False\n",
      "was be AUX VBD ROOT xxx True True\n",
      "even even ADV RB advmod xxxx True True\n",
      "better well ADJ JJR acomp xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "the the DET DT det xxx True True\n",
      "story story NOUN NN nsubj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "childhood childhood NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "pain pain NOUN NN conj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "revenge revenge NOUN NN conj xxxx True False\n",
      "was be VERB VBD conj xxx True True\n",
      "interesting interesting ADJ JJ acomp xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "moving move VERB VBG conj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "A a DET DT det X True True\n",
      "superior superior ADJ JJ amod xxxx True False\n",
      "hollywood hollywood NOUN NN compound xxxx True False\n",
      "film film NOUN NN ROOT xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "But but CCONJ CC ROOT Xxx True True\n",
      "... ... PUNCT NFP punct ... False False\n",
      "No no DET DT det Xx True True\n",
      "one one NOUN NN nsubj xxx True True\n",
      "mentioned mention VERB VBD ccomp xxxx True False\n",
      "this this DET DT dobj xxxx True True\n",
      "so so ADV RB advmod xx True True\n",
      "far far ADV RB advmod xxx True False\n",
      "( ( PUNCT -LRB- punct ( False False\n",
      "at at ADP IN advmod xx True True\n",
      "least least ADJ JJS advmod xxxx True True\n",
      "in in ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "latest late ADJ JJS amod xxxx True False\n",
      "20 20 NUM CD nummod dd False False\n",
      "comments comment NOUN NNS pobj xxxx True False\n",
      ") ) PUNCT -RRB- punct ) False False\n",
      ", , PUNCT , punct , False False\n",
      "when when ADV WRB advmod xxxx True True\n",
      "it it PRON PRP nsubj xx True True\n",
      "came come VERB VBD advcl xxxx True False\n",
      "to to ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "courtroom courtroom NOUN NN compound xxxx True False\n",
      "scenes scene NOUN NNS pobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "Brat Brat PROPN NNP nmod Xxxx True False\n",
      "Pitt´s pitt´s NUM CD nummod Xxxx´x False False\n",
      "character character NOUN NN nsubj xxxx True False\n",
      "followed follow VERB VBD ccomp xxxx True False\n",
      "his his PRON PRP$ poss xxx True True\n",
      "plan plan NOUN NN dobj xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "rescue rescue VERB VB acl xxxx True False\n",
      "his his PRON PRP$ poss xxx True True\n",
      "two two NUM CD nummod xxx True True\n",
      "friends friend NOUN NNS dobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "who who PRON WP nsubjpass xxx True True\n",
      "are be AUX VBP auxpass xxx True True\n",
      "rightly rightly ADV RB advmod xxxx True False\n",
      "accused accuse VERB VBN relcl xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "murder murder NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "i I PRON PRP nsubj x True True\n",
      "felt feel VERB VBD ROOT xxxx True False\n",
      "cheated cheated ADJ JJ acomp xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "This this DET DT det Xxxx True True\n",
      "movie movie NOUN NN nsubj xxxx True False\n",
      "insulted insult VERB VBD ROOT xxxx True False\n",
      "my my PRON PRP$ poss xx True True\n",
      "intelligence intelligence NOUN NN dobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "< < X XX dep < False False\n",
      "br br VERB VBP ROOT xx True False\n",
      "/><br /><br X XX punct /><xx False False\n",
      "/>Warning />warne VERB VBG prep />Xxxxx False False\n",
      "spoilers!!<br spoilers!!<br NOUN NNS dobj xxxx!!<xx False False\n",
      "/><br /><br NOUN NNS dobj /><xx False False\n",
      "/>Why />Why PUNCT . punct />Xxx False False\n",
      "did do AUX VBD aux xxx True True\n",
      "anyone anyone PRON NN nsubj xxxx True True\n",
      "accept accept VERB VB ROOT xxxx True False\n",
      "their their PRON PRP$ poss xxxx True True\n",
      "false false ADJ JJ amod xxxx True False\n",
      "alibi alibi NOUN NN dobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "witnessed witness VERB VBN acl xxxx True False\n",
      "by by ADP IN agent xx True True\n",
      "the the DET DT det xxx True True\n",
      "priest priest NOUN NN pobj xxxx True False\n",
      "? ? PUNCT . punct ? False False\n",
      "If if SCONJ IN mark Xx True True\n",
      "these these DET DT det xxxx True True\n",
      "two two NUM CD nummod xxx True True\n",
      "guys guy NOUN NNS nsubj xxxx True False\n",
      "had have AUX VBD aux xxx True True\n",
      "been be VERB VBN advcl xxxx True True\n",
      "with with ADP IN prep xxxx True True\n",
      "him he PRON PRP pobj xxx True True\n",
      ", , PUNCT , punct , False False\n",
      "why why ADV WRB advmod xxx True True\n",
      "shouldn´t shouldn´t ADV RB advmod xxxx´x False False\n",
      "they they PRON PRP nsubj xxxx True True\n",
      "tell tell VERB VBP ROOT xxxx True False\n",
      "this this DET DT dobj xxxx True True\n",
      "during during ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "investigation investigation NOUN NN pobj xxxx True False\n",
      "? ? PUNCT . punct ? False False\n",
      "Amnesia Amnesia PROPN NNP ROOT Xxxxx True False\n",
      "? ? PUNCT . punct ? False False\n",
      "If if SCONJ IN mark Xx True True\n",
      "you you PRON PRP nsubj xxx True True\n",
      "were be VERB VBD advcl xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "judge judge NOUN NN attr xxxx True False\n",
      "or or CCONJ CC cc xx True True\n",
      "member member NOUN NN conj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "jury jury NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "would would AUX MD aux xxxx True True\n",
      "you you PRON PRP nsubj xxx True True\n",
      "believe believe VERB VB ROOT xxxx True False\n",
      "it it PRON PRP dobj xx True True\n",
      "? ? PUNCT . punct ? False False\n",
      "Is be AUX VBZ ROOT Xx True True\n",
      "it it PRON PRP nsubj xx True True\n",
      "wise wise ADJ JJ acomp xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "give give VERB VB xcomp xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "motif motif NOUN NN dobj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "murderers murderer NOUN NNS pobj xxxx True False\n",
      "away?<br away?<br PROPN NNP dep xxxx?<xx False False\n",
      "/><br /><br NOUN NNS punct /><xx False False\n",
      "/>I />i NOUN NN nsubj />X False False\n",
      "am be AUX VBP ccomp xx True True\n",
      "sorry sorry ADJ JJ acomp xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "but but CCONJ CC cc xxx True True\n",
      "in in ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "end end NOUN NN pobj xxx True False\n",
      ", , PUNCT , punct , False False\n",
      "the the DET DT det xxx True True\n",
      "story story NOUN NN nsubj xxxx True False\n",
      "is be VERB VBZ conj xx True True\n",
      "very very ADV RB advmod xxxx True True\n",
      "weak weak ADJ JJ acomp xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "and and CCONJ CC cc xxx True True\n",
      "this this DET DT nsubj xxxx True True\n",
      "angers anger VERB VBZ conj xxxx True False\n",
      "me I PRON PRP dobj xx True True\n",
      ". . PUNCT . punct . False False\n",
      "This this DET DT det Xxxx True True\n",
      "movie movie NOUN NN nsubj xxxx True False\n",
      "had have VERB VBD ROOT xxx True True\n",
      "great great ADJ JJ amod xxxx True False\n",
      "potential potential NOUN NN dobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "4/10 4/10 NUM CD ROOT d/dd False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i I PRON PRP nsubj x True True\n",
      "ve ve VERB VBP ROOT xx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"ive\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "['Prs']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "token = doc[0]  # 'I'\n",
    "print(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\n",
    "print(token.morph.get(\"PronType\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diz o pronome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case=Nom|Person=2|PronType=Prs\n",
      "PRON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Where are you?\")\n",
    "print(doc[2].morph)  # 'Case=Nom|Person=2|PronType=Prs'\n",
    "print(doc[2].pos_)  # 'PRON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemastização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule\n",
      "['ca', 'nt']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# English pipelines include a rule-based lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "\n",
    "doc = nlp(\"cant\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca ROOT ca AUX [nt]\n",
      "nt punct ca AUX []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"cant\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \t PREFIX\n",
      "Let \t SPECIAL-1\n",
      "'s \t SPECIAL-2\n",
      "go \t TOKEN\n",
      "! \t SUFFIX\n",
      "\" \t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''\"Let's go!\"'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\t\", t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('''\"Let's go!\"''')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.', ':)', \"can't\", \"i'll\", \"i've\", 'ht', \"'\", 'jj', \"'\", '<br', '/><br', '/>']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                url_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(\"hello-world. :) can't i'll i've ht ' jj' <br /><br />\")\n",
    "print([t.text for t in doc]) # ['hello', '-', 'world.', ':)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1769b557610>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
